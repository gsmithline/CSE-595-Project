{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from contrastive_loss import ContrastiveLoss\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SourceNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyLinear(128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class RefNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyLinear(128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_pairs(json_file):\n",
    "    rows = []\n",
    "    \n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for paper in data:\n",
    "        paper_id = paper['_id']\n",
    "        \n",
    "        # Get the referenced paper IDs from refs_trace\n",
    "        positive_refs = [ref['_id'] for ref in paper.get('refs_trace', [])]\n",
    "        \n",
    "        # Create rows for positive examples\n",
    "        for ref_id in positive_refs:\n",
    "            rows.append({\n",
    "                'source_id': paper_id,\n",
    "                'ref_id': ref_id,\n",
    "                'target': 1\n",
    "            })\n",
    "        \n",
    "        # Create rows for negative examples\n",
    "        for ref_id in paper['references']:\n",
    "            if ref_id not in positive_refs:\n",
    "                rows.append({\n",
    "                    'source_id': paper_id,\n",
    "                    'ref_id': ref_id,\n",
    "                    'target': 0\n",
    "                })\n",
    "            \n",
    "                \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create both positive and negative pairs\n",
    "df = create_pairs('data/paper_source_trace_train_ans.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_embeddings = pd.read_csv('data/abstract_embeddings.csv')\n",
    "title_embeddings = pd.read_csv('data/title_embeddings.csv')\n",
    "\n",
    "# Get all unique paper IDs from both source and ref columns\n",
    "all_paper_ids = pd.concat([df['source_id'], df['ref_id']]).unique()\n",
    "\n",
    "# Get list of paper IDs that exist in abs_embeddings\n",
    "valid_ids = set(abs_embeddings['id'].values)\n",
    "\n",
    "# Filter df to only keep rows where both source_id and ref_id are in abs_embeddings\n",
    "df = df[df['source_id'].isin(valid_ids) & df['ref_id'].isin(valid_ids)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset class\n",
    "class PaperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pairs_df, abs_embeddings_df, title_embeddings_df):\n",
    "        self.pairs_df = pairs_df\n",
    "        \n",
    "        # Create combined embeddings dictionary\n",
    "        self.embeddings = {}\n",
    "        for _, row in abs_embeddings_df.iterrows():\n",
    "            paper_id = row['id']\n",
    "            abs_emb = row[row.index[1:]].values.astype(np.float32)\n",
    "            title_emb = title_embeddings_df[title_embeddings_df['id'] == paper_id].iloc[0][title_embeddings_df.columns[1:]].values.astype(np.float32)\n",
    "            combined_emb = np.concatenate([abs_emb, title_emb])\n",
    "            self.embeddings[paper_id] = torch.tensor(combined_emb, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs_df.iloc[idx]\n",
    "        source_emb = self.embeddings[row['source_id']]\n",
    "        ref_emb = self.embeddings[row['ref_id']]\n",
    "        label = torch.tensor(row['target'], dtype=torch.float32)\n",
    "        return source_emb, ref_emb, label\n",
    "\n",
    "# Create dataset\n",
    "dataset = PaperDataset(df, abs_embeddings, title_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "batch_size = 32\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1154\n",
      "Epoch [2/100], Loss: 0.1103\n",
      "Epoch [3/100], Loss: 0.1067\n",
      "Epoch [4/100], Loss: 0.1062\n",
      "Epoch [5/100], Loss: 0.1056\n",
      "Epoch [6/100], Loss: 0.1061\n",
      "Epoch [7/100], Loss: 0.1088\n",
      "Epoch [8/100], Loss: 0.1051\n",
      "Epoch [9/100], Loss: 0.1023\n",
      "Epoch [10/100], Loss: 0.1035\n",
      "Epoch [11/100], Loss: 0.1038\n",
      "Epoch [12/100], Loss: 0.1027\n",
      "Epoch [13/100], Loss: 0.1030\n",
      "Epoch [14/100], Loss: 0.1020\n",
      "Epoch [15/100], Loss: 0.1012\n",
      "Epoch [16/100], Loss: 0.1019\n",
      "Epoch [17/100], Loss: 0.1030\n",
      "Epoch [18/100], Loss: 0.1021\n",
      "Epoch [19/100], Loss: 0.1016\n",
      "Epoch [20/100], Loss: 0.1014\n",
      "Epoch [21/100], Loss: 0.1008\n",
      "Epoch [22/100], Loss: 0.1031\n",
      "Epoch [23/100], Loss: 0.1030\n",
      "Epoch [24/100], Loss: 0.1061\n",
      "Epoch [25/100], Loss: 0.1037\n",
      "Epoch [26/100], Loss: 0.1037\n",
      "Epoch [27/100], Loss: 0.1005\n",
      "Epoch [28/100], Loss: 0.1003\n",
      "Epoch [29/100], Loss: 0.1002\n",
      "Epoch [30/100], Loss: 0.0999\n",
      "Epoch [31/100], Loss: 0.0998\n",
      "Epoch [32/100], Loss: 0.1008\n",
      "Epoch [33/100], Loss: 0.0997\n",
      "Epoch [34/100], Loss: 0.1021\n",
      "Epoch [35/100], Loss: 0.1037\n",
      "Epoch [36/100], Loss: 0.1069\n",
      "Epoch [37/100], Loss: 0.1009\n",
      "Epoch [38/100], Loss: 0.1023\n",
      "Epoch [39/100], Loss: 0.1006\n",
      "Epoch [40/100], Loss: 0.0989\n",
      "Epoch [41/100], Loss: 0.0999\n",
      "Epoch [42/100], Loss: 0.1016\n",
      "Epoch [43/100], Loss: 0.1009\n",
      "Epoch [44/100], Loss: 0.0980\n",
      "Epoch [45/100], Loss: 0.0997\n",
      "Epoch [46/100], Loss: 0.0998\n",
      "Epoch [47/100], Loss: 0.0996\n",
      "Epoch [48/100], Loss: 0.0988\n",
      "Epoch [49/100], Loss: 0.1000\n",
      "Epoch [50/100], Loss: 0.0981\n",
      "Epoch [51/100], Loss: 0.0995\n",
      "Epoch [52/100], Loss: 0.1073\n",
      "Epoch [53/100], Loss: 0.1025\n",
      "Epoch [54/100], Loss: 0.1009\n",
      "Epoch [55/100], Loss: 0.0986\n",
      "Epoch [56/100], Loss: 0.1000\n",
      "Epoch [57/100], Loss: 0.1000\n",
      "Epoch [58/100], Loss: 0.0986\n",
      "Epoch [59/100], Loss: 0.0977\n",
      "Epoch [60/100], Loss: 0.0981\n",
      "Epoch [61/100], Loss: 0.1000\n",
      "Epoch [62/100], Loss: 0.1026\n",
      "Epoch [63/100], Loss: 0.1000\n",
      "Epoch [64/100], Loss: 0.0992\n",
      "Epoch [65/100], Loss: 0.0983\n",
      "Epoch [66/100], Loss: 0.0969\n",
      "Epoch [67/100], Loss: 0.0965\n",
      "Epoch [68/100], Loss: 0.1000\n",
      "Epoch [69/100], Loss: 0.0972\n",
      "Epoch [70/100], Loss: 0.0989\n",
      "Epoch [71/100], Loss: 0.0959\n",
      "Epoch [72/100], Loss: 0.0994\n",
      "Epoch [73/100], Loss: 0.0978\n",
      "Epoch [74/100], Loss: 0.0965\n",
      "Epoch [75/100], Loss: 0.0965\n",
      "Epoch [76/100], Loss: 0.0958\n",
      "Epoch [77/100], Loss: 0.0964\n",
      "Epoch [78/100], Loss: 0.0984\n",
      "Epoch [79/100], Loss: 0.0972\n",
      "Epoch [80/100], Loss: 0.0967\n",
      "Epoch [81/100], Loss: 0.0944\n",
      "Epoch [82/100], Loss: 0.0963\n",
      "Epoch [83/100], Loss: 0.0941\n",
      "Epoch [84/100], Loss: 0.0967\n",
      "Epoch [85/100], Loss: 0.0966\n",
      "Epoch [86/100], Loss: 0.0961\n",
      "Epoch [87/100], Loss: 0.0963\n",
      "Epoch [88/100], Loss: 0.0939\n",
      "Epoch [89/100], Loss: 0.0947\n",
      "Epoch [90/100], Loss: 0.0950\n",
      "Epoch [91/100], Loss: 0.0955\n",
      "Epoch [92/100], Loss: 0.0935\n",
      "Epoch [93/100], Loss: 0.0936\n",
      "Epoch [94/100], Loss: 0.0947\n",
      "Epoch [95/100], Loss: 0.0936\n",
      "Epoch [96/100], Loss: 0.0949\n",
      "Epoch [97/100], Loss: 0.0941\n",
      "Epoch [98/100], Loss: 0.0920\n",
      "Epoch [99/100], Loss: 0.0960\n",
      "Epoch [100/100], Loss: 0.0918\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize networks and optimizer\n",
    "source_net = SourceNet()\n",
    "ref_net = RefNet()\n",
    "optimizer = torch.optim.Adam(list(source_net.parameters()) + list(ref_net.parameters()), lr=1e-4)\n",
    "\n",
    "# Contrastive loss function\n",
    "def contrastive_loss(source_out, ref_out, labels, margin=1.0):\n",
    "    distances = F.pairwise_distance(source_out, ref_out)\n",
    "    return torch.mean((1-labels) * torch.pow(distances, 2) + \n",
    "                     labels * torch.pow(torch.clamp(margin - distances, min=0.0), 2))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for source_emb, ref_emb, labels in dataloader:\n",
    "        # Forward pass\n",
    "        source_out = source_net(source_emb)\n",
    "        ref_out = ref_net(ref_emb)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = contrastive_loss(source_out, ref_out, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance for target pairs: 0.0639\n",
      "Average distance for non-target pairs: 0.1307\n"
     ]
    }
   ],
   "source": [
    "# Calculate average distances for target and non-target pairs\n",
    "target_distances = []\n",
    "nontarget_distances = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for source_emb, ref_emb, labels in dataloader:\n",
    "        # Get network outputs\n",
    "        source_out = source_net(source_emb)\n",
    "        ref_out = ref_net(ref_emb)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = F.pairwise_distance(source_out, ref_out)\n",
    "        \n",
    "        # Split distances based on labels\n",
    "        target_distances.extend(distances[labels == 0].tolist())\n",
    "        nontarget_distances.extend(distances[labels == 1].tolist())\n",
    "\n",
    "avg_target_dist = sum(target_distances) / len(target_distances)\n",
    "avg_nontarget_dist = sum(nontarget_distances) / len(nontarget_distances)\n",
    "\n",
    "print(f\"Average distance for target pairs: {avg_target_dist:.4f}\")\n",
    "print(f\"Average distance for non-target pairs: {avg_nontarget_dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process validation data\n",
    "with open('data/paper_source_trace_valid_wo_ans.json', 'r') as f:\n",
    "    valid_data = json.load(f)\n",
    "\n",
    "# Load submission example for padding reference\n",
    "with open('data/submission_example_valid.json', 'r') as f:\n",
    "    submission_example = json.load(f)\n",
    "\n",
    "embeddings_dict = dataset.embeddings\n",
    "paper_scores = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for paper in valid_data:\n",
    "        paper_id = paper['_id']\n",
    "        references = paper['references']\n",
    "        expected_length = len(submission_example[paper_id])\n",
    "        \n",
    "        if paper_id not in embeddings_dict:\n",
    "            paper_scores[paper_id] = [0.0] * expected_length\n",
    "            continue\n",
    "        \n",
    "        source_out = source_net(embeddings_dict[paper_id].unsqueeze(0))\n",
    "        \n",
    "        scores = []\n",
    "        for ref_id in references[:expected_length]:\n",
    "            if ref_id not in embeddings_dict:\n",
    "                scores.append(0.0)\n",
    "                continue\n",
    "            \n",
    "            ref_out = ref_net(embeddings_dict[ref_id].unsqueeze(0))\n",
    "            distance = F.pairwise_distance(source_out, ref_out).item()\n",
    "            score = max(0, min(1, 1 - distance))  # Clip score between 0 and 1\n",
    "            scores.append(score)\n",
    "        \n",
    "        scores += [0.0] * (expected_length - len(scores))\n",
    "        paper_scores[paper_id] = scores\n",
    "\n",
    "with open('validation_scores.json', 'w') as f:\n",
    "    json.dump(paper_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare lengths with submission example\n",
    "with open('data/submission_example_valid.json', 'r') as f:\n",
    "    submission_example = json.load(f)\n",
    "\n",
    "for paper_id, scores in paper_scores.items():\n",
    "    example_scores = submission_example[paper_id]\n",
    "    if len(scores) != len(example_scores):\n",
    "        print(f\"{paper_id}: scores={len(scores)}, example={len(example_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
