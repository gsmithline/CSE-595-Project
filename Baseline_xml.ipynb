{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%unload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import numpy; print(numpy.__version__)\"\n",
    "!python -c \"import pandas; print(pandas.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "#scibert with rules .7366998112933661\n",
    "#scibert no rules 0.6866199738713892\n",
    "#bert with rules  0.7386957468427929\n",
    "#bert no rules 0.6786543765423143\n",
    "openai = False\n",
    "\n",
    "    \n",
    "\n",
    "#file_path = '/Users/gabesmithline/Desktop/gnn_project/data/adj_matrix_scibert_no_rules.pkl'\n",
    "#file_path = 'adj_matrix.pkl'\n",
    "file_path = \"../gnn_project/papers_missing_all_refs.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    G1 = pickle.load(file)\n",
    "print(G1)\n",
    "if openai:\n",
    "    file_path = '../gnn_project/data/adj_matrix_openai_no_rules.pkl'\n",
    "else:\n",
    "    file_path = '../gnn_project/data/adj_matrix.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    G = pickle.load(file)\n",
    "print(G.nodes())\n",
    "    #remove papers in G1 from G\n",
    "for node in G1:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node)\n",
    "        print(f\"removed node {node}\")\n",
    "#print node attributes\n",
    "#for node in G.nodes(data=True):\n",
    "#     print(*node)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for node in G.nodes(data=True):\n",
    "#     print(*node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding_df = pd.read_csv('data/title_embeddings.csv',index_col=0) if not openai else pd.read_csv('data/title_embeddings_openai.csv',index_col=0)\n",
    "# title_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if openai: #construct openai embeddings\n",
    "    paper_embeddings_df = os.path.join(\"data/paper_embed_openai\")\n",
    "    files = os.listdir(paper_embeddings_df)\n",
    "    for file in files:\n",
    "        with open(os.path.join(paper_embeddings_df, file), \"rb\") as f:\n",
    "            \n",
    "            paper_embeddings = np.load(f, allow_pickle=True)\n",
    "            print(paper_embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abs_embedding_df = pd.read_csv('data/abstract_embeddings.csv',index_col=0) if not openai else pd.read_csv('data/abstract_embeddings_openai.csv',index_col=0)\n",
    "def load_openai_paper_sources():\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    data = []\n",
    "    directory = os.path.join(\"data/paper_embed_openai\")\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(os.path.join(directory, file), \"rb\") as f:\n",
    "                paper_source = np.load(f, allow_pickle=True)\n",
    "                \n",
    "                if isinstance(paper_source, (tuple, list)) and len(paper_source) == 2:\n",
    "                    _id, embedding = paper_source\n",
    "                elif isinstance(paper_source, dict):\n",
    "                    _id = paper_source.get('_id', file)\n",
    "                    embedding = paper_source.get('embedding') or paper_source.get('paper_embeddings', paper_source)\n",
    "                else:\n",
    "                    _id = file  \n",
    "                    embedding = paper_source\n",
    "                \n",
    "\n",
    "                if isinstance(_id, str) and _id.endswith(\".npy\"):\n",
    "                    _id = _id[:-4] \n",
    "                \n",
    "                data.append({\n",
    "                    \"_id\": _id,\n",
    "                    \"paper_embeddings\": embedding\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file}: {e}\")\n",
    "    \n",
    "    embeddings = pd.DataFrame(data)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "openai_embeddings = load_openai_paper_sources()\n",
    "print(openai_embeddings.head())\n",
    "print(openai_embeddings.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in G.nodes():\n",
    "    if node == \"6218ac7a5aee126c0f614f1a\":\n",
    "        print(G.nodes[node])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for node in G.nodes():\n",
    "    print(node)\n",
    "    if not openai and node in title_embedding_df.index and node in abs_embedding_df.index:\n",
    "        G.nodes[node]['title_embeddings'] = torch.tensor(title_embedding_df.loc[node].values)\n",
    "        G.nodes[node]['abstract_embeddings'] = torch.tensor(abs_embedding_df.loc[node].values)\n",
    "        G.nodes[node]['paper_id'] = node\n",
    "    elif openai and node in openai_embeddings[\"_id\"].values:\n",
    "        G.nodes[node]['paper_embeddings'] = torch.tensor(openai_embeddings.loc[openai_embeddings[\"_id\"] == node][\"paper_embeddings\"].tolist())\n",
    "        G.nodes[node]['paper_id'] = node\n",
    "    else:\n",
    "        G.nodes[node]['paper_id'] = node\n",
    "        count+=1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name_idx_dict = {}\n",
    "idx = 0\n",
    "for node in G.nodes(data=True):\n",
    "    print(node[0])\n",
    "    node_name_idx_dict[node[0]] = idx\n",
    "    idx+=1\n",
    "node_name_idx_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.relabel_nodes(G, node_name_idx_dict)\n",
    "G.nodes()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citation_info(xml_path):\n",
    "   \n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    citation_info = {}\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    paragraphs = root.findall('.//tei:body//tei:p', ns)\n",
    "\n",
    "    full_text = ''.join(ET.tostring(e, encoding='unicode', method='text') for e in paragraphs)\n",
    "    total_length = len(full_text)\n",
    "\n",
    "    current_length = 0  \n",
    "\n",
    "    for para in paragraphs:\n",
    "    \n",
    "        para_text = ET.tostring(para, encoding='unicode', method='text')\n",
    "        para_length = len(para_text)\n",
    "\n",
    "        refs_in_para = para.findall('.//tei:ref', ns)\n",
    "\n",
    "        for ref in refs_in_para:\n",
    "            ref_id = ref.get('target', '').lstrip('#')\n",
    "            if not ref_id:\n",
    "                continue  \n",
    "\n",
    "        \n",
    "            position_normalized = current_length / total_length if total_length > 0 else 0.0\n",
    "\n",
    "            if ref_id not in citation_info:\n",
    "                citation_info[ref_id] = {\n",
    "                    'count': 1,\n",
    "                    'positions': [position_normalized]\n",
    "                }\n",
    "            else:\n",
    "                citation_info[ref_id]['count'] += 1\n",
    "                citation_info[ref_id]['positions'].append(position_normalized)\n",
    "\n",
    "        current_length += para_length\n",
    "\n",
    "    return citation_info\n",
    "\n",
    "def map_refs_to_paper_ids(xml_path):\n",
    "   \n",
    "    ref_to_paper_id = {}\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    bibl_structs = root.findall('.//tei:back//tei:listBibl//tei:biblStruct', ns)\n",
    "    for bibl in bibl_structs:\n",
    "        ref_id = bibl.get('{http://www.w3.org/XML/1998/namespace}id')\n",
    "        if ref_id:\n",
    "            doi_elem = bibl.find('.//tei:idno[@type=\"DOI\"]', ns)\n",
    "            if doi_elem is not None and doi_elem.text:\n",
    "                paper_id = doi_elem.text.strip()\n",
    "            else:\n",
    "                title_elem = bibl.find('.//tei:title[@level=\"a\"]', ns)\n",
    "                if title_elem is not None and title_elem.text:\n",
    "                    paper_id = title_elem.text.strip()\n",
    "                else:\n",
    "                    paper_id = ref_id \n",
    "            print(f\"ref_id: {ref_id}, paper_id: {paper_id}\")\n",
    "            ref_to_paper_id[ref_id] = paper_id\n",
    "\n",
    "    return ref_to_paper_id\n",
    "\n",
    "def get_citation_features(xml_path):\n",
    "    \n",
    "    citation_info = extract_citation_info(xml_path)\n",
    "    ref_to_paper_id = map_refs_to_paper_ids(xml_path)\n",
    "\n",
    "    citation_features = {}\n",
    "    for ref_id, info in citation_info.items():\n",
    "        paper_id = ref_to_paper_id.get(ref_id)\n",
    "        if paper_id:\n",
    "            avg_position = sum(info['positions']) / len(info['positions'])\n",
    "            citation_features[paper_id] = {\n",
    "                'count': info['count'],\n",
    "                'avg_position': avg_position\n",
    "            }\n",
    "    return citation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_citation_features = {}\n",
    "\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    paper_id = node[1]['paper_id']\n",
    "    xml_path = f'data/paper-xml/{paper_id}.xml'  \n",
    "    print(xml_path)\n",
    "    if os.path.exists(xml_path):\n",
    "        citation_features = get_citation_features(xml_path)\n",
    "        all_citation_features[paper_id] = citation_features\n",
    "        print(citation_features)\n",
    "    else:\n",
    "        print(f\"XML file for {paper_id} does not exist\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper_ids = [node[1]['paper_id'] for node in G.nodes(data=True)]\n",
    "print(paper_ids)\n",
    "paper_id_to_index = {paper_id: idx for idx, paper_id in enumerate(paper_ids)}\n",
    "\n",
    "edge_counts = []\n",
    "edge_positions = []\n",
    "edge_indices = []\n",
    "for edge in G.edges(data=True):\n",
    "    print(edge)\n",
    "    edge_attributes = edge[2]\n",
    "    src_paper_id, dst_paper_id = edge_attributes[\"source\"], edge_attributes[\"target\"]\n",
    "    src_idx = paper_id_to_index[src_paper_id]\n",
    "    dst_idx = paper_id_to_index[dst_paper_id]\n",
    "\n",
    "    citation_features = all_citation_features.get(src_paper_id, {})\n",
    "    features = citation_features.get(dst_paper_id, {'count': 0, 'avg_position': 0.0})\n",
    "\n",
    "    edge_counts.append(features['count'])\n",
    "    edge_positions.append(features['avg_position'])\n",
    "    edge_indices.append([src_idx, dst_idx])\n",
    "\n",
    "edge_index = torch.tensor(edge_indices).t().contiguous()\n",
    "edge_counts = torch.tensor(edge_counts, dtype=torch.float).view(-1, 1)\n",
    "edge_positions = torch.tensor(edge_positions, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "# Concatenate edge attributes\n",
    "edge_attr = torch.cat([edge_counts, edge_positions], dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title_embedding_df = pd.read_csv('data/title_embeddings.csv', index_col=0)\n",
    "abstract_embedding_df = pd.read_csv('data/abstract_embeddings.csv', index_col=0)\n",
    "\n",
    "node_features = []\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    abstract_embedding = torch.zeros(768, dtype=torch.float)\n",
    "    title_embedding = torch.zeros(768, dtype=torch.float)\n",
    "    paper_id = node[1]['paper_id']\n",
    "    # Get title embedding\n",
    "    if paper_id in title_embedding_df.index:\n",
    "        print(f\"paper_id {paper_id} found in title_embedding_df\")\n",
    "        title_embedding = torch.tensor(title_embedding_df.loc[paper_id].values, dtype=torch.float)\n",
    "    else:\n",
    "        print(f\"paper_id {paper_id} not found in title_embedding_df\")\n",
    "        #title_embedding = torch.zeros(768, dtype=torch.float)\n",
    "    \n",
    "    if paper_id in abstract_embedding_df.index:\n",
    "        print(f\"paper_id {paper_id} found in abstract_embedding_df\")\n",
    "        abstract_embedding = torch.tensor(abstract_embedding_df.loc[paper_id].values, dtype=torch.float)\n",
    "    else:\n",
    "        print(f\"paper_id {paper_id} not found in abstract_embedding_df\")\n",
    "        abstract_embedding = torch.zeros(768, dtype=torch.float)\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    if abstract_embedding.sum() == 0 and title_embedding.sum() == 0:\n",
    "        print(f\"paper_id {paper_id} has no embeddings\")\n",
    "    else:\n",
    "        combined_features = torch.cat([title_embedding, abstract_embedding])\n",
    "        node_features.append(combined_features)\n",
    "\n",
    "node_features_tensor = torch.stack(node_features)\n",
    "print(node_features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(\n",
    "    x=node_features_tensor,\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_label_dict = {}  \n",
    "\n",
    "edge_labels = []\n",
    "for idx in range(edge_index.size(1)):\n",
    "    src_idx = edge_index[0, idx].item()\n",
    "    dst_idx = edge_index[1, idx].item()\n",
    "    src_paper_id = G.nodes[src_idx]['paper_id']\n",
    "    dst_paper_id = G.nodes[dst_idx]['paper_id']\n",
    "\n",
    "    label = link_label_dict.get((src_paper_id, dst_paper_id), 0)\n",
    "    edge_labels.append(label)\n",
    "\n",
    "edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "data.edge_label = edge_labels\n",
    "data.edge_label_index = data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_edges = data.edge_index.size(1)\n",
    "num_val = int(num_edges * 0.1)\n",
    "num_test = int(num_edges * 0.2)\n",
    "num_train = num_edges - num_val - num_test\n",
    "\n",
    "\n",
    "perm = torch.randperm(num_edges)\n",
    "train_idx = perm[:num_train]\n",
    "val_idx = perm[num_train:num_train + num_val]\n",
    "test_idx = perm[num_train + num_val:]\n",
    "\n",
    "#CHECK IF DATA CONTAINS NANs\n",
    "if torch.isnan(data.x).any():\n",
    "    print(\"NaN detected in node features\")\n",
    "else:\n",
    "    print(\"No NaN in node features\")\n",
    "if torch.isnan(data.edge_index).any():\n",
    "    print(\"NaN detected in edge index\")\n",
    "else:\n",
    "    print(\"No NaN in edge index\")\n",
    "if torch.isnan(data.edge_attr).any():\n",
    "    print(\"NaN detected in edge attributes\")\n",
    "else:\n",
    "    print(\"No NaN in edge attributes\")\n",
    "from copy import deepcopy\n",
    "\n",
    "train_data = deepcopy(data)\n",
    "train_data.edge_index = data.edge_index[:, train_idx]\n",
    "train_data.edge_attr = data.edge_attr[train_idx]\n",
    "train_data.edge_label = data.edge_label[train_idx]\n",
    "train_data.edge_label_index = data.edge_index[:, train_idx]\n",
    "\n",
    "val_data = deepcopy(data)\n",
    "val_data.edge_index = data.edge_index[:, val_idx]\n",
    "val_data.edge_attr = data.edge_attr[val_idx]\n",
    "val_data.edge_label = data.edge_label[val_idx]\n",
    "val_data.edge_label_index = data.edge_index[:, val_idx]\n",
    "\n",
    "test_data = deepcopy(data)\n",
    "test_data.edge_index = data.edge_index[:, test_idx]\n",
    "test_data.edge_attr = data.edge_attr[test_idx]\n",
    "test_data.edge_label = data.edge_label[test_idx]\n",
    "test_data.edge_label_index = data.edge_index[:, test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_edge_attributes(edge_attr):\n",
    "    \"\"\"\n",
    "    Normalizes edge attributes to have zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    mean = edge_attr.mean(dim=0, keepdim=True)\n",
    "    std = edge_attr.std(dim=0, keepdim=True)\n",
    "    std[std < 1e-6] = 1.0  # Prevent division by zero\n",
    "    return (edge_attr - mean) / std\n",
    "\n",
    "#edge_attr = normalize_edge_attributes(edge_attr)\n",
    "\n",
    "if torch.isnan(edge_attr).any() or torch.isinf(edge_attr).any():\n",
    "    raise ValueError(\"Invalid values detected in edge_attr after normalization.\")\n",
    "\n",
    "print(\"Edge attributes normalized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import Module\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNModel(Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, edge_attr_dim):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        \n",
    "        edge_mlp_input_dim = out_channels * 2 + edge_attr_dim  # 2*128 + 2 = 258\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(edge_mlp_input_dim, 64),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(64, 1) \n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        z = self.encode(\n",
    "            data.x.to(device), \n",
    "            data.edge_index.to(device), \n",
    "            data.edge_attr.to(device)\n",
    "        )\n",
    "        \n",
    "        out = self.decode(\n",
    "            z, \n",
    "            data.edge_label_index.to(device), \n",
    "            data.edge_attr.to(device)\n",
    "        )\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def normalize_node_features(self, x):\n",
    "        print(\"Before normalization - min:\", x.min().item(), \"max:\", x.max().item(), \"mean:\", x.mean().item())\n",
    "        \n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True)\n",
    "        std[std < 1e-6] = 1.0 \n",
    "        x_normalized = (x - mean) / std\n",
    "        x_normalized = torch.clamp(x_normalized, min=-5.0, max=5.0)\n",
    "        \n",
    "        print(\"After normalization - min:\", x_normalized.min().item(), \"max:\", x_normalized.max().item(), \"mean:\", x_normalized.mean().item())\n",
    "        \n",
    "        if torch.isnan(x_normalized).any():\n",
    "            print(\"NaNs detected in normalized x\")\n",
    "        return x_normalized\n",
    "    \n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        x = self.normalize_node_features(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x, negative_slope=0.1)  # Using LeakyReLU\n",
    "        x = torch.clamp(x, min=-10.0, max=10.0)\n",
    "        \n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected after conv1 and activation/clamp\")\n",
    "            return x  # Early return for debugging\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x, negative_slope=0.1)  # Using LeakyReLU\n",
    "        # Clamping to prevent exploding values\n",
    "        x = torch.clamp(x, min=-10.0, max=10.0)\n",
    "        \n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected after conv2 and activation/clamp\")\n",
    "            return x  \n",
    "        \n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index, edge_attr):\n",
    "        src = z[edge_label_index[0]]  \n",
    "        dst = z[edge_label_index[1]]  \n",
    "        \n",
    "        print(f\"src shape: {src.shape}\")\n",
    "        print(f\"dst shape: {dst.shape}\")\n",
    "        print(f\"edge_attr shape: {edge_attr.shape}\")\n",
    "        print(f\"edge_label_index shape: {edge_label_index.shape}\")\n",
    "        \n",
    "        if edge_attr.size(0) != edge_label_index.size(1):\n",
    "            repeat_times = edge_label_index.size(1) // edge_attr.size(0)\n",
    "            remainder = edge_label_index.size(1) % edge_attr.size(0)\n",
    "            \n",
    "            if remainder == 0:\n",
    "                edge_attr = edge_attr.repeat(repeat_times, 1)\n",
    "            else:\n",
    "                edge_attr = torch.cat([\n",
    "                    edge_attr.repeat(repeat_times, 1),\n",
    "                    edge_attr[:remainder]\n",
    "                ], dim=0)\n",
    "        \n",
    "        # Verify shapes after adjustment\n",
    "        assert src.size(0) == dst.size(0) == edge_attr.size(0), \\\n",
    "            f\"Shape mismatch: src={src.shape}, dst={dst.shape}, edge_attr={edge_attr.shape}\"\n",
    "        \n",
    "        combined = torch.cat([src, dst, edge_attr], dim=1)\n",
    "        score = self.edge_mlp(combined)\n",
    "        return score.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCNModel(\n",
    "    in_channels=1536,        \n",
    "    hidden_channels=128,\n",
    "    out_channels=128,\n",
    "    edge_attr_dim=2          \n",
    ").to(device)\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, GCNConv):\n",
    "            torch.nn.init.kaiming_uniform_(m.lin.weight, nonlinearity='leaky_relu')\n",
    "            if m.lin.bias is not None:\n",
    "                torch.nn.init.zeros_(m.lin.bias)\n",
    "        elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "            torch.nn.init.ones_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "# After model creation\n",
    "initialize_weights(model)\n",
    "print(\"Weights initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(\n",
    "    x=node_features_tensor,\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr,\n",
    ")\n",
    "\n",
    "print(\"Before split:\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Max edge index: {data.edge_index.max().item()}\")\n",
    "\n",
    "\n",
    "def verify_and_fix_data(data):\n",
    "    num_nodes = data.x.size(0)\n",
    "    valid_edges_mask = (data.edge_index[0] < num_nodes) & (data.edge_index[1] < num_nodes)\n",
    "    \n",
    "    data.edge_index = data.edge_index[:, valid_edges_mask]\n",
    "    data.edge_attr = data.edge_attr[valid_edges_mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = verify_and_fix_data(data)\n",
    "\n",
    "print(\"\\nAfter verifying and fixing edge indices:\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Max edge index: {data.edge_index.max().item()}\")\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,          # 10% for validation\n",
    "    num_test=0.2,         # 20% for testing\n",
    "    is_undirected=True,  \n",
    "    add_negative_train_samples=True\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(f\"\\nOriginal number of nodes in G: {G.number_of_nodes()}\")\n",
    "print(f\"Original number of edges in G: {G.number_of_edges()}\")\n",
    "\n",
    "print(\"\\nAfter split:\")\n",
    "print(f\"Train data - Nodes: {train_data.num_nodes}, Edges: {train_data.num_edges}, Max edge index: {train_data.edge_index.max().item()}\")\n",
    "print(f\"Val data - Nodes: {val_data.num_nodes}, Edges: {val_data.num_edges}, Max edge index: {val_data.edge_index.max().item()}\")\n",
    "print(f\"Test data - Nodes: {test_data.num_nodes}, Edges: {test_data.num_edges}, Max edge index: {test_data.edge_index.max().item()}\")\n",
    "\n",
    "assert train_data.edge_index.max().item() < train_data.num_nodes, \"Invalid edge indices in train data\"\n",
    "assert val_data.edge_index.max().item() < val_data.num_nodes, \"Invalid edge indices in val data\"\n",
    "assert test_data.edge_index.max().item() < test_data.num_nodes, \"Invalid edge indices in test data\"\n",
    "\n",
    "print(\"\\nData splitting successful with consistent edge indices.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  \n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "pos_weight = (train_data.edge_label == 0).sum() / (train_data.edge_label == 1).sum()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    eps = 1e-7\n",
    "    \n",
    "    out = model(data)\n",
    "    out = torch.clamp(out, min=1e-7, max=1-1e-7)  # Prevent extreme values\n",
    "        \n",
    "    print(f\"Output range: min={out.min().item()}, max={out.max().item()}\")\n",
    "    print(f\"Labels range: min={data.edge_label.min().item()}, max={data.edge_label.max().item()}\")\n",
    " \n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "    individual_losses = criterion(out, data.edge_label.float().to(device))\n",
    "\n",
    "    if torch.isnan(individual_losses).any():\n",
    "        print(\"NaN detected in individual losses\")\n",
    "        individual_losses = torch.nan_to_num(individual_losses, nan=0.0)\n",
    "\n",
    "    loss = individual_losses.mean()\n",
    "    loss.backward()\n",
    "  \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad = torch.nan_to_num(param.grad, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    data = data.to(device)  \n",
    "    \n",
    "    z = model.encode(data.x, data.edge_index, data.edge_attr)\n",
    "    \n",
    "\n",
    "    if torch.isnan(z).any():\n",
    "        print(\"NaN detected in node embeddings z\")\n",
    "        return 0.0, 0.0\n",
    "    else:\n",
    "        print(\"No NaN in node embeddings z\")\n",
    "    \n",
    "    out_raw = model.decode(z, data.edge_label_index, data.edge_attr)\n",
    "    \n",
    "    if torch.isnan(out_raw).any():\n",
    "        print(\"NaN detected in raw output of model.decode\")\n",
    "        return 0.0, 0.0\n",
    "    else:\n",
    "        print(\"No NaN in raw output of model.decode\")\n",
    "    \n",
    "    out = out_raw.sigmoid()\n",
    "    \n",
    "    if torch.isnan(out).any():\n",
    "        print(\"NaN detected in output after sigmoid\")\n",
    "        return 0.0, 0.0\n",
    "    else:\n",
    "        print(\"No NaN in output after sigmoid\")\n",
    "    \n",
    "    print(f\"Shape of out: {out.shape}\") \n",
    "    print(f\"Shape of data.edge_label: {data.edge_label.shape}\")  \n",
    "    \n",
    "    if out.shape != data.edge_label.shape:\n",
    "        print(\"Shape mismatch between predictions and labels.\")\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    pred = out >= 0.5\n",
    "    acc = pred.eq(data.edge_label.to(device).bool()).sum().item() / data.edge_label.size(0)\n",
    "    \n",
    "    from sklearn.metrics import average_precision_score\n",
    "    y_true = data.edge_label.cpu().numpy()\n",
    "    y_scores = out.cpu().detach().numpy()\n",
    "    \n",
    "    if np.isnan(y_true).any():\n",
    "        print(\"NaN detected in y_true\")\n",
    "        y_true = np.nan_to_num(y_true, nan=0.0)\n",
    "    \n",
    "    if np.isnan(y_scores).any():\n",
    "        print(\"NaN detected in y_scores\")\n",
    "        y_scores = np.nan_to_num(y_scores, nan=0.0)\n",
    "    \n",
    "    map_score = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    return acc, map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial number of nodes: {node_features_tensor.shape[0]}\")\n",
    "print(f\"Initial edge_index shape: {edge_index.shape}\")\n",
    "print(f\"Max node index in edge_index: {edge_index.max().item()}\")\n",
    "\n",
    "# Get unique node indices from edge_index\n",
    "unique_nodes_in_edges = torch.unique(edge_index)\n",
    "print(f\"Unique nodes in edge_index: {len(unique_nodes_in_edges)}\")\n",
    "\n",
    "num_nodes = node_features_tensor.shape[0]\n",
    "valid_nodes = set(range(num_nodes))\n",
    "nodes_in_edges = set(unique_nodes_in_edges.tolist())\n",
    "valid_edge_nodes = nodes_in_edges.intersection(valid_nodes)\n",
    "\n",
    "node_idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted(valid_edge_nodes))}\n",
    "\n",
    "valid_edges_mask = torch.tensor([\n",
    "    (edge_index[0, i].item() in node_idx_map) and (edge_index[1, i].item() in node_idx_map)\n",
    "    for i in range(edge_index.shape[1])\n",
    "])\n",
    "\n",
    "edge_index = edge_index[:, valid_edges_mask]\n",
    "edge_attr = edge_attr[valid_edges_mask]\n",
    "\n",
    "edge_index_list = edge_index.tolist()\n",
    "edge_index = torch.tensor([\n",
    "    [node_idx_map[src] for src in edge_index_list[0]],\n",
    "    [node_idx_map[dst] for dst in edge_index_list[1]]\n",
    "])\n",
    "\n",
    "new_node_features = torch.stack([node_features_tensor[i] for i in sorted(valid_edge_nodes)])\n",
    "\n",
    "# Update node_features_tensor\n",
    "node_features_tensor = new_node_features\n",
    "\n",
    "# Verify the fix\n",
    "print(f\"\\nAfter fixing:\")\n",
    "print(f\"Number of nodes: {node_features_tensor.shape[0]}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Max node index in edge_index: {edge_index.max().item()}\")\n",
    "assert edge_index.max() < node_features_tensor.shape[0], \"Edge indices still contain invalid references\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "best_loss = float('inf')\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "map_list = []\n",
    "test_acc_list = []\n",
    "test_map_list = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "   #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Reduced from 1.0\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)  # Added weight decay\n",
    "    \n",
    "    loss = train(train_data)\n",
    "    \n",
    "    if torch.isnan(torch.tensor(loss)) or torch.isinf(torch.tensor(loss)):\n",
    "        print(f\"Epoch {epoch}: Loss is {loss}. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        print(f\"New best loss: {loss:.4f}\")\n",
    "        best_loss = loss\n",
    "    \n",
    "    print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    val_acc, val_map = evaluate(val_data)\n",
    "    acc_list.append(val_acc)\n",
    "    map_list.append(val_map)\n",
    "    test_acc, test_map = evaluate(test_data)\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_map_list.append(test_map)\n",
    "    print(f'Metrics - Val Acc: {val_acc:.4f}, Val MAP: {val_map:.4f}, Test Acc: {test_acc:.4f}, Test MAP: {test_map:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "display.display(plt.gcf())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acc_list, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "display.display(plt.gcf())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(map_list, label='Validation MAP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAP')\n",
    "plt.title('Validation MAP Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "display.display(plt.gcf())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_map(predictions, ground_truths):\n",
    "\n",
    "    combined = list(zip(predictions, ground_truths))\n",
    "    print(\"Combined Predictions and Ground Truths:\", combined)\n",
    "    \n",
    "    sorted_combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    print(\"Sorted Combined List:\", sorted_combined)\n",
    "    \n",
    "    den = len(sorted_combined)\n",
    "    print(\"Total Number of Predictions (den):\", den)\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for rank, (_, gt) in enumerate(sorted_combined, start=1):\n",
    "        if gt == 1:  \n",
    "            contribution = 1 - (rank - 1) / den\n",
    "            score += contribution\n",
    "            print(f\"Rank: {rank}, GT: {gt}, Contribution: {contribution}, Cumulative Score: {score}\")\n",
    "    \n",
    "    num_positives = sum(ground_truths) \n",
    "    print(\"Number of Positives:\", num_positives) \n",
    "    #if there are no positive edges, return 0, the number of ground truth should be at most the number of predictions\n",
    "    map_score = score / min(len(predictions), num_positives) if num_positives > 0 else 0\n",
    "    print(\"Calculated MAP Score:\", map_score)\n",
    "    \n",
    "    return map_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model(test_data)\n",
    "print(f\"test_predictions: {len(test_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_ground_truth = test_data.edge_label.cpu().numpy()[:len(test_predictions)]\n",
    "map_score = custom_map(test_predictions, test_predictions_ground_truth)\n",
    "print(f\"Custom MAP score: {map_score:.4f}\") \n",
    "torch.save(test_predictions, 'test_predictions.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
