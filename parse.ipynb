{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_title_and_abstract(file_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define the XML namespaces\n",
    "    namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Extract the title\n",
    "    title_elem = root.find('.//tei:titleStmt/tei:title[@level=\"a\"][@type=\"main\"]', namespaces)\n",
    "    title = title_elem.text if title_elem is not None and title_elem.text else \"Title not found\"\n",
    "\n",
    "    # Extract the abstract\n",
    "    abstract_elem = root.find('.//tei:div/tei:p', namespaces)\n",
    "    abstract = abstract_elem.text if abstract_elem is not None else \"Abstract not found\"\n",
    "\n",
    "    return title, abstract\n",
    "\n",
    "# Usage\n",
    "file_path = \"/Users/gabesmithline/Desktop/gnn_project/data/paper-xml/5a4aef6f17c44a2190f7877f.xml\"\n",
    "title, abstract = extract_title_and_abstract(file_path)\n",
    "print(\"Title:\", title)\n",
    "print(\"\\nAbstract:\", abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the title and abstract\n",
    "inputs = tokenizer(title + \" \" + abstract, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "# Run the input through BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the pooled output (representation of the entire input)\n",
    "pooled_output = outputs.pooler_output\n",
    "\n",
    "print(\"BERT embedding shape:\", pooled_output.shape)\n",
    "print(\"First few values of the embedding:\", pooled_output[0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=True))\n",
    "\n",
    "def process_xml_files(directory):\n",
    "    under_512 = 0\n",
    "    over_512 = 0\n",
    "    total_files = 0\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                _, abstract = extract_title_and_abstract(file_path)\n",
    "                token_count = count_tokens(abstract)\n",
    "                \n",
    "                if token_count <= 512:\n",
    "                    under_512 += 1\n",
    "                else:\n",
    "                    over_512 += 1\n",
    "                \n",
    "                total_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "    return under_512, over_512, total_files\n",
    "\n",
    "# Process all XML files in the paper-xml directory\n",
    "paper_xml_dir = \"paper-xml\"\n",
    "under_512, over_512, total_files = process_xml_files(paper_xml_dir)\n",
    "\n",
    "print(f\"Total files processed: {total_files}\")\n",
    "print(f\"Abstracts with 512 or fewer tokens: {under_512}\")\n",
    "print(f\"Abstracts with more than 512 tokens: {over_512}\")\n",
    "print(f\"Percentage of abstracts over 512 tokens: {(over_512 / total_files) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.pooler_output.squeeze().numpy()\n",
    "\n",
    "def process_and_embed_xml_files(directory):\n",
    "    abstract_embeddings = {}\n",
    "    title_embeddings = {}\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                title, abstract = extract_title_and_abstract(file_path)\n",
    "                \n",
    "                # Embed title\n",
    "                title_embedding = embed_text(title)\n",
    "                title_embeddings[filename[:-4]] = title_embedding\n",
    "                \n",
    "                # Embed abstract\n",
    "                abstract_embedding = embed_text(abstract)\n",
    "                abstract_embeddings[filename[:-4]] = abstract_embedding\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "    return title_embeddings, abstract_embeddings\n",
    "\n",
    "# Process and embed all XML files in the paper-xml directory\n",
    "paper_xml_dir = \"paper-xml\"\n",
    "title_embeddings, abstract_embeddings = process_and_embed_xml_files(paper_xml_dir)\n",
    "\n",
    "# Convert embeddings to DataFrames\n",
    "title_df = pd.DataFrame.from_dict(title_embeddings, orient='index')\n",
    "abstract_df = pd.DataFrame.from_dict(abstract_embeddings, orient='index')\n",
    "\n",
    "# Save embeddings to CSV files\n",
    "title_df.to_csv('title_embeddings.csv')\n",
    "abstract_df.to_csv('abstract_embeddings.csv')\n",
    "\n",
    "print(\"Title embeddings saved to 'title_embeddings.csv'\")\n",
    "print(\"Abstract embeddings saved to 'abstract_embeddings.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(200, 10)\n",
    "# Normalize the tensor by row\n",
    "a_normalized = torch.nn.functional.normalize(a, p=1, dim=1)\n",
    "\n",
    "# Define temperature for the concrete distribution\n",
    "temperature = 0.1\n",
    "\n",
    "# Sample from the concrete distribution\n",
    "gumbel_noise = -torch.log(-torch.log(torch.rand_like(a_normalized)))\n",
    "gumbel_max_samples = torch.argmax(torch.log(a_normalized) + gumbel_noise, dim=1)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "samples = torch.zeros_like(a_normalized)\n",
    "samples.scatter_(1, gumbel_max_samples.unsqueeze(1), 1)\n",
    "\n",
    "print(\"Shape of samples:\", samples.shape)\n",
    "print(\"Sum of each row:\", samples.sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
